{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice on Applying RNN/LSTM model on imdb sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#import necessary modules\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overvies of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features we want\n",
    "#num_words: integer or None. Top most frequent words to consider. \n",
    "#Any less frequent word will appear as oov_char value in the sequence data.\n",
    "number_of_features = None\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape, train_target.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The longest comment in training set\n",
    "max([len(train_data[i]) for i in range(len(train_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 3s 2us/step\n"
     ]
    }
   ],
   "source": [
    "#get the word_index_dict\n",
    "word_index_dict = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index_word_dict\n",
    "index_word_dict = dict()\n",
    "for k,v in word_index_dict.items():\n",
    "    index_word_dict[v] = k\n",
    "len(index_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's see what the comment is\n",
    "def get_comment(dataset, index):\n",
    "    comment = []\n",
    "    for w in dataset[index]:\n",
    "        comment.append(index_word_dict[w])\n",
    "    return ' '.join(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the thought solid thought senator do making to is spot nomination assumed while he of jack in where picked as getting on was did hands fact characters to always life thrillers not as me can't in at are br of sure your way of little it strongly random to view of love it so principles of guy it used producer of where it of here icon film of outside to don't all unique some like of direction it if out her imagination below keep of queen he diverse to makes this stretch stefan of solid it thought begins br senator machinations budget worthwhile though ok brokedown awaiting for ever better were lugia diverse for budget look kicked any to of making it out bosworth's follows for effects show to show cast this family us scenes more it severe making senator to levant's finds tv tend to of emerged these thing wants but fuher an beckinsale cult as it is video do you david see scenery it in few those are of ship for with of wild to one is very work dark they don't do dvd with those them\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comment(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorical_labels = to_categorical(train_target, num_classes=None)\n",
    "#categorical_labels.shape # only 2 categories(0/1)\n",
    "#del(categorical_labels)\n",
    "train_target[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is very small if we want to apply deep learning RNN model on it. And the size of the training set is the same as the test set, both are 25,000. Although it is a small dataset, the longest comment has more than 2 thousand words, and the comments includes 88,584 unique words, which might make the RNN model hard to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the length of each comment is different, I would like to limit the length and make every comment have the same length. To decide which maxlen I should pick, I would like to do some digging to see the features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comment_length(dataset):\n",
    "    length_list = np.array([len(i) for i in dataset])\n",
    "    return length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std:  176.49367364852034\n",
      "Mean:  238.71364\n",
      "Median:  178.0\n"
     ]
    }
   ],
   "source": [
    "train_length_data = get_comment_length(train_data)\n",
    "print('Std: ', train_length_data.std())\n",
    "print('Mean: ', train_length_data.mean()) \n",
    "print('Median: ', np.percentile(train_length_data, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzet4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11daa5438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "plt.boxplot(train_length_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_up_limit = np.percentile(train_length_data, 75) + 1.5*(np.percentile(train_length_data, 75) - \n",
    "                                                             np.percentile(train_length_data, 25))\n",
    "train_up_limit #about 93% \n",
    "#np.percentile(train_length_data, 93) #538.0\n",
    "#np.percentile(train_length_data, 99) #926.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88585"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count word usage\n",
    "from collections import defaultdict\n",
    "def word_counter(dataset):\n",
    "    wc = defaultdict(int)\n",
    "    for i in range(len(dataset)):\n",
    "        for j in dataset[i]:\n",
    "            wc[j] += 1\n",
    "    return wc\n",
    "len(word_counter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 1000 counts:  612\n",
      "More than 500 counts:  1157\n",
      "More than 100 counts:  4311\n"
     ]
    }
   ],
   "source": [
    "def usage_calculate(wc):\n",
    "    above_100 = []\n",
    "    above_500 = []\n",
    "    above_1000 = []\n",
    "    for k,v in wc.items():\n",
    "        if v >= 1000:\n",
    "            above_1000.append(k)\n",
    "        elif v >= 500:\n",
    "            above_500.append(k)\n",
    "        elif v >= 100:\n",
    "            above_100.append(k)\n",
    "    return above_1000, above_500, above_100\n",
    "        \n",
    "above_1000, above_500, above_100 = usage_calculate(word_counter(train_data))\n",
    "print('More than 1000 counts: ', len(above_1000))\n",
    "print('More than 500 counts: ', len(above_1000)+len(above_500))\n",
    "print('More than 100 counts: ', len(above_1000)+len(above_500)+len(above_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some digging, I find that the length of comments is diverse. Although the shortest comment has only 11 words, the median and standard deviation are 178 and 176.5 respectively. Also, only 4311 words are counted more than 100 times while there are total 88,585 words used in commenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and fast try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use words which have more than 500 counts \n",
    "number_of_features = 1157\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce the word using\n",
    "len(word_counter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use padding or truncation to make each observation have 532 features (up outlier limit)\n",
    "train_features = sequence.pad_sequences(train_data, maxlen=532)\n",
    "test_features = sequence.pad_sequences(test_data, maxlen=532)\n",
    "len(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(number_of_features, output_dims = 128, units_num = 128):\n",
    "    # Start neural network\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # Add an embedding layer\n",
    "    network.add(layers.Embedding(input_dim=number_of_features, output_dim=output_dims))\n",
    "\n",
    "    # Add a long short-term memory layer with 128 units\n",
    "    network.add(layers.LSTM(units=units_num))\n",
    "\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile neural network\n",
    "    network.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                    optimizer='Adam',     # Adam optimization\n",
    "                    metrics=['accuracy']) # Accuracy performance metric\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = lstm_model(number_of_features, output_dims = 128, units_num = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 784s 31ms/step - loss: 0.6068 - acc: 0.6617 - val_loss: 0.4618 - val_acc: 0.7843\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 800s 32ms/step - loss: 0.4429 - acc: 0.8194 - val_loss: 0.4045 - val_acc: 0.8255\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 3941s 158ms/step - loss: 0.3568 - acc: 0.8526 - val_loss: 0.3391 - val_acc: 0.8588\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 733s 29ms/step - loss: 0.3382 - acc: 0.8589 - val_loss: 0.3415 - val_acc: 0.8554\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 945s 38ms/step - loss: 0.3056 - acc: 0.8744 - val_loss: 0.3141 - val_acc: 0.8676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1289a5668>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_features, train_target,\n",
    "           epochs = 5,\n",
    "           verbose=1,               # Get the progressing bar in each epoch\n",
    "           batch_size=256,          # Number of observations per batch\n",
    "           validation_data=(test_features, test_target)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://chrisalbon.com/deep_learning/keras/lstm_recurrent_neural_network/\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
